{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "https://www.kaggle.com/datasets/jhoward/lsun_bedroom/data\n",
    "\n",
    "@misc{yu2016lsun,\n",
    "      title={LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop}, \n",
    "      author={Fisher Yu and Ari Seff and Yinda Zhang and Shuran Song and Thomas Funkhouser and Jianxiong Xiao},\n",
    "      year={2016},\n",
    "      eprint={1506.03365},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_path = './data/subset'\n",
    "#data_path = './data/data0/lsun/bedroom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size after transformations\n",
    "image_size = 64\n",
    "\n",
    "simple_load = v2.Compose([\n",
    "    v2.Resize((image_size, image_size)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM\n",
    "\n",
    "https://huggi64ace.co/docs/diffusers/en/tutorials/basic_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maaja\\repos\\DeepLearning\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from diffusers.utils import make_image_grid\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3270e410910345c49b7aa2ce884f3dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/303125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(data_path, split='train')\n",
    "dataset.set_transform(simple_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    unet_model = UNet2DModel(\n",
    "        sample_size=image_size,  # the target image resolution\n",
    "        in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "        out_channels=3,  # the number of output channels\n",
    "        layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    ).to(device)\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, pipeline, save_name: str = 'ddpm_training', random_state: int | None = None):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=16,\n",
    "        generator=torch.manual_seed(random_state),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join('saved', save_name)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, n_epochs: int, noise_scheduler, optimizer, dataset, batch_size: int = 16, start_epoch: int = 0,\n",
    "               save_every: int = 1, save_name: str = 'ddpm_training'):\n",
    "    data_loader = DataLoader(dataset, batch_size)\n",
    "\n",
    "    for i in range(start_epoch, n_epochs + start_epoch):\n",
    "        epoch_no = i + 1\n",
    "        print(f'Starting epoch {epoch_no}...')\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "\n",
    "            # move to cuda/cpu\n",
    "            clean_images = batch[\"image\"].to(device)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluate every nth epoch\n",
    "        if epoch_no % save_every == 0:\n",
    "            print(f'Evaluating after epoch {epoch_no}...')\n",
    "            # save model\n",
    "            save_dir = f'saved/{save_name}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            model.save_pretrained(f'{save_dir}/{epoch_no:04d}_model')\n",
    "            # evaluate and save images\n",
    "            pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "            evaluate(epoch_no, pipeline, save_name, random_state=epoch_no)\n",
    "        \n",
    "        # cooldown\n",
    "        time.sleep(60 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-3}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=5, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters()), # default lr = 0.001\n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_lr1e-3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-4}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 84/18946 [00:44<2:46:44,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=20, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=0.0001), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_lr1e-4',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-5}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet( \n",
    "    unet_model, \n",
    "    n_epochs=10, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-5), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_lr1e-5',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=20, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-4, weight_decay=1e-3), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_reg1e-3',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 84/18946 [00:44<2:46:44,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=20, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-4, weight_decay=1e-4), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_reg1e-4',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.vision.gan import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.losses import *\n",
    "from fastai.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, path):\n",
    "    class TorchVisionTransform(Transform):\n",
    "        def __init__(self, tfms): self.tfms = tfms\n",
    "        def encodes(self, img: PILImage): return self.tfms(img)\n",
    "        \n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock),\n",
    "        get_items=get_image_files,\n",
    "        splitter=FuncSplitter(lambda x: False),\n",
    "        item_tfms=TorchVisionTransform(simple_load)\n",
    "    )\n",
    "    \n",
    "    dls = dblock.dataloaders(path, bs=batch_size) \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dls = get_data(batch_size, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (64*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (64*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (64*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (64) x 32 x 32\n",
    "            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (3) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # input is (3) x 64 x 64\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (64) x 32 x 32\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (64*2) x 16 x 16\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (64*4) x 8 x 8\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (64*8) x 4 x 4\n",
    "            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch, generator, save_name: str = 'gan_training'):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    noise = torch.randn(16, 100, 1, 1, device=device)\n",
    "\n",
    "    batch = generator(noise)\n",
    "    pil_images = []\n",
    "    for tensor in batch:\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "        tensor = (tensor * 0.5) + 0.5 #denormalize\n",
    "        tensor = (tensor * 255).byte()\n",
    "        pil_image = Image.fromarray(tensor.numpy())\n",
    "        pil_images.append(pil_image)\n",
    "    image_grid = make_image_grid(pil_images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join('saved', save_name)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have the necessary imports, data loading, and model definitions from the previous code\n",
    "\n",
    "# Training function for vanilla GAN or DCGAN\n",
    "def train_gan(generator, discriminator, dls, opt_gen, opt_disc, n_epochs, \n",
    "              start_epoch: int = 0, save_every: int = 1, save_name: str = 'gan_training'):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for i in range(start_epoch, n_epochs + start_epoch):\n",
    "        epoch_no = i + 1\n",
    "\n",
    "        for real in tqdm(dls[0]):\n",
    "            real = real[0]\n",
    "            real = real.to(device)\n",
    "            batch_size = real.size(0)\n",
    "            \n",
    "            # Train discriminator\n",
    "            opt_disc.zero_grad()\n",
    "            noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "\n",
    "            fake = generator(noise)\n",
    "            disc_real = discriminator(real)\n",
    "            disc_fake = discriminator(fake.detach())\n",
    "            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "            loss_disc.backward()\n",
    "            opt_disc.step()\n",
    "            \n",
    "            # Train generator\n",
    "            opt_gen.zero_grad()\n",
    "            disc_fake = discriminator(fake)\n",
    "            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch_no} done! Loss discriminator: {loss_disc.item():.4f}, Loss generator: {loss_gen.item():.4f}\")\n",
    "        # evaluate every nth epoch\n",
    "        if epoch_no % save_every == 0:\n",
    "            print(f'Evaluating after epoch {epoch_no}...')\n",
    "            # save model\n",
    "            save_dir = f'saved/{save_name}'\n",
    "            model_save_dir = f'{save_dir}/{epoch_no:04d}_model'\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "            generator_path = os.path.join(model_save_dir, 'generator.pth')\n",
    "            discriminator_path = os.path.join(model_save_dir, 'discriminator.pth')\n",
    "            torch.save(generator.state_dict(), generator_path)\n",
    "            torch.save(discriminator.state_dict(), discriminator_path)\n",
    "            \n",
    "            evaluate(epoch_no, generator, save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan(path):\n",
    "    generator = Generator().to(device)\n",
    "    generator.load_state_dict(torch.load(f\"{path}/generator.pth\", map_location=torch.device(device)))\n",
    "\n",
    "    discriminator = Discriminator().to(device)\n",
    "    discriminator.load_state_dict(torch.load(f\"{path}/discriminator.pth\", map_location=torch.device(device)))\n",
    "    \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:11<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done! Loss D: 0.5032, Loss G: 0.6931\n",
      "Evaluating after epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:11<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done! Loss D: 0.5032, Loss G: 0.6931\n",
      "Evaluating after epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:12<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done! Loss D: 0.5032, Loss G: 0.6931\n",
      "Evaluating after epoch 3...\n"
     ]
    }
   ],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-3}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=0.001)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=0.001)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_lr1e-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-4}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-4)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_lr1e-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-5}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_lr1e-5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
