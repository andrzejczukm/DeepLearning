{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "https://www.kaggle.com/datasets/jhoward/lsun_bedroom/data\n",
    "\n",
    "@misc{yu2016lsun,\n",
    "      title={LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop}, \n",
    "      author={Fisher Yu and Ari Seff and Yinda Zhang and Shuran Song and Thomas Funkhouser and Jianxiong Xiao},\n",
    "      year={2016},\n",
    "      eprint={1506.03365},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data_path = './data/subset'\n",
    "data_path = './data/data0/lsun/bedroom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image size after transformations\n",
    "image_size = 64\n",
    "\n",
    "simple_load = v2.Compose([\n",
    "    v2.Resize((image_size, image_size)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM\n",
    "\n",
    "https://huggi64ace.co/docs/diffusers/en/tutorials/basic_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "from diffusers.utils import make_image_grid\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997f23b239e84cd696e8aefa382a4de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/303125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(data_path, split='train')\n",
    "dataset.set_transform(simple_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    unet_model = UNet2DModel(\n",
    "        sample_size=image_size,  # the target image resolution\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "        ),\n",
    "    ).to(device)\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ddpm(epoch, pipeline, save_name: str = 'ddpm_training', random_state: int | None = None):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=16,\n",
    "        generator=torch.manual_seed(random_state),\n",
    "    ).images\n",
    "\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "    test_dir = os.path.join('saved', save_name)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, n_epochs: int, noise_scheduler, optimizer, dataset, batch_size: int = 16, start_epoch: int = 0,\n",
    "               save_every: int = 1, save_name: str = 'ddpm_training'):\n",
    "    data_loader = DataLoader(dataset, batch_size)\n",
    "\n",
    "    for i in range(start_epoch - 1, n_epochs + start_epoch - 1):\n",
    "        epoch_no = i + 1\n",
    "        print(f'Starting epoch {epoch_no}...')\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "            clean_images = batch[\"image\"].to(device)\n",
    "            noise = torch.randn(clean_images.shape, device=device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        if epoch_no % save_every == 0:\n",
    "            print(f'Evaluating after epoch {epoch_no}...')\n",
    "            save_dir = f'saved/{save_name}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            model.save_pretrained(f'{save_dir}/{epoch_no:04d}_model')\n",
    "            # evaluate and save images\n",
    "            pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "            evaluate_ddpm(epoch_no, pipeline, save_name, random_state=epoch_no)\n",
    "        \n",
    "        time.sleep(60 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-3}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=5, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters()), # default lr = 0.001\n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_lr1e-3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-4}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 84/18946 [00:44<2:46:44,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=20, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=0.0001), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_lr1e-4',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-5}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet( \n",
    "    unet_model, \n",
    "    n_epochs=10, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-5), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_lr1e-5',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18946/18946 [2:41:53<00:00,  1.95it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after epoch 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22801b73253848d992eb20b7394765dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\utils\\pil_utils.py:43: RuntimeWarning: invalid value encountered in cast\n",
      "  images = (images * 255).round().astype(\"uint8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18946/18946 [2:37:46<00:00,  2.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after epoch 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b407a878926141c3ab6419217151ea2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18946/18946 [2:37:54<00:00,  2.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after epoch 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bee038f2b34db292c30dafb3b03ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=3, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-4, weight_decay=1e-4), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_reg1e-4',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 84/18946 [00:44<2:46:44,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=3, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-4, weight_decay=1e-5), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_reg1e-5',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18946/18946 [2:34:50<00:00,  2.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after epoch 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aafc6b278d943a5a19d78bdc7527d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18946/18946 [2:35:37<00:00,  2.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after epoch 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3278b8f12ee8444c9be08bb2a5e59127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18946/18946 [2:37:28<00:00,  2.01it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after epoch 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc16ee6eaad4676b86d4dd22845d55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unet_model = get_unet()\n",
    "train_unet(\n",
    "    unet_model, \n",
    "    n_epochs=3, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    optimizer=optim.Adam(unet_model.parameters(), lr=1e-4, weight_decay=1e-6), \n",
    "    dataset=dataset, \n",
    "    batch_size=16,\n",
    "    save_every=1,\n",
    "    save_name='ddpm_training_reg1e-6',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately using regularisation made all images all black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.vision.gan import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.losses import *\n",
    "from fastai.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, path):\n",
    "    class TorchVisionTransform(Transform):\n",
    "        def __init__(self, tfms): self.tfms = tfms\n",
    "        def encodes(self, img: PILImage): return self.tfms(img)\n",
    "        \n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock),\n",
    "        get_items=get_image_files,\n",
    "        splitter=FuncSplitter(lambda x: False),\n",
    "        item_tfms=TorchVisionTransform(simple_load)\n",
    "    )\n",
    "    \n",
    "    dls = dblock.dataloaders(path, bs=batch_size) \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dls = get_data(batch_size, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( 64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( 64 * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.network(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gan(epoch, generator, save_name: str = 'gan_training'):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    noise = torch.randn(16, 100, 1, 1, device=device)\n",
    "\n",
    "    batch = generator(noise)\n",
    "    pil_images = []\n",
    "    for tensor in batch:\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "        tensor = (tensor * 0.5) + 0.5 #denormalize\n",
    "        tensor = (tensor * 255).byte()\n",
    "        pil_image = Image.fromarray(tensor.to('cpu').numpy())\n",
    "        pil_images.append(pil_image)\n",
    "    image_grid = make_image_grid(pil_images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join('saved', save_name)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, dls, opt_gen, opt_disc, n_epochs, \n",
    "              start_epoch: int = 0, save_every: int = 1, save_name: str = 'gan_training'):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for i in range(start_epoch - 1, n_epochs + start_epoch - 1):\n",
    "        epoch_no = i + 1\n",
    "\n",
    "        for real in tqdm(dls[0]):\n",
    "            real = real[0]\n",
    "            real = real.to(device)\n",
    "            batch_size = real.size(0)\n",
    "            \n",
    "            # Train discriminator\n",
    "            opt_disc.zero_grad()\n",
    "            noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "\n",
    "            fake = generator(noise)\n",
    "            disc_real = discriminator(real)\n",
    "            disc_fake = discriminator(fake.detach())\n",
    "            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "            loss_disc.backward()\n",
    "            opt_disc.step()\n",
    "            \n",
    "            # Train generator\n",
    "            opt_gen.zero_grad()\n",
    "            disc_fake = discriminator(fake)\n",
    "            loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch_no} done! Loss discriminator: {loss_disc.item():.4f}, Loss generator: {loss_gen.item():.4f}\")\n",
    "        if epoch_no % save_every == 0:\n",
    "            print(f'Evaluating after epoch {epoch_no}...')\n",
    "\n",
    "            save_dir = f'saved/{save_name}'\n",
    "            model_save_dir = f'{save_dir}/{epoch_no:04d}_model'\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "            generator_path = os.path.join(model_save_dir, 'generator.pth')\n",
    "            discriminator_path = os.path.join(model_save_dir, 'discriminator.pth')\n",
    "            torch.save(generator.state_dict(), generator_path)\n",
    "            torch.save(discriminator.state_dict(), discriminator_path)\n",
    "            \n",
    "            evaluate_gan(epoch_no, generator, save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gan(path):\n",
    "    generator = Generator().to(device)\n",
    "    generator.load_state_dict(torch.load(f\"{path}/generator.pth\", map_location=torch.device(device)))\n",
    "\n",
    "    discriminator = Discriminator().to(device)\n",
    "    discriminator.load_state_dict(torch.load(f\"{path}/discriminator.pth\", map_location=torch.device(device)))\n",
    "    \n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-3}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:44<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:43<00:00, 16.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:18<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:35<00:00, 16.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:53<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:32<00:00, 16.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:19<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:37<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:34<00:00, 16.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:27<00:00, 16.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 done! Loss discriminator: 0.8133, Loss generator: 0.3133\n",
      "Evaluating after epoch 10...\n"
     ]
    }
   ],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=0.001)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=0.001)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_lr1e-3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-4}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:06<00:00, 16.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:17<00:00, 16.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:35<00:00, 16.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:57<00:00, 15.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:09<00:00, 16.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:20<00:00, 16.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:10<00:00, 16.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:04<00:00, 16.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:17<00:00, 16.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:11<00:00, 16.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 10...\n"
     ]
    }
   ],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-4)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_lr1e-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning rate $10^{-5}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [20:23<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:26<00:00, 16.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:26<00:00, 16.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 done! Loss discriminator: 0.5034, Loss generator: 0.6928\n",
      "Evaluating after epoch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:32<00:00, 16.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:36<00:00, 16.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 done! Loss discriminator: 0.5033, Loss generator: 0.6931\n",
      "Evaluating after epoch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:24<00:00, 16.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:37<00:00, 16.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:48<00:00, 15.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:28<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18945/18945 [19:23<00:00, 16.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 done! Loss discriminator: 0.5032, Loss generator: 0.6931\n",
      "Evaluating after epoch 21...\n"
     ]
    }
   ],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, start_epoch=11, save_name = 'gan_training_lr1e-5-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-3)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_reg_1e-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_reg_1e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_reg_1e-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_reg_1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing batch-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dls = get_data(batch_size, data_path)\n",
    "\n",
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_batch-size_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dls = get_data(batch_size, data_path)\n",
    "\n",
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_batch-size_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dls = get_data(batch_size, data_path)\n",
    "\n",
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_batch-size_32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dls = get_data(batch_size, data_path)\n",
    "\n",
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_batch-size_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dls = get_data(batch_size, data_path)\n",
    "\n",
    "model_generator = Generator().to(device)\n",
    "model_discriminator = Discriminator().to(device)\n",
    "\n",
    "opt_gen = optim.Adam(model_generator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "opt_disc = optim.Adam(model_discriminator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "train_gan(model_generator, model_discriminator, dls, opt_gen, opt_disc, n_epochs=10, save_name = 'gan_training_batch-size_128')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
